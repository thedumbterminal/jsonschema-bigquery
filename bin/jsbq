#!/usr/bin/env node

const gbq = require('../src/gbq')
const { get_table_id } = require('../src/utils')
const converter = require('../src/converter')
const { logger } = require('../src/log')
const fs = require('fs')
const { promisify } = require('util')
const yargs = require('yargs')
const hideBin = require('yargs/helpers').hideBin

const jsbq = (module.exports = {})

jsbq.process = async (project, datasetName, jsonSchema, options) => {
  logger.info('Processing JSON schema...')
  const tableOptions = converter.run(jsonSchema, options)
  logger.info('Generated BigQuery schema:')
  console.log(JSON.stringify(tableOptions.schema.fields))

  if (!project && !datasetName) {
    return logger.info('Skipping table operations')
  }

  const schemaId = jsonSchema.$id || jsonSchema.id
  logger.info('Extracting table name from schema ID:', schemaId)
  const tableName = get_table_id(schemaId)
  logger.info('Table name:', tableName)

  logger.info('Setting table options...')
  tableOptions.friendly_name = jsonSchema.title
  tableOptions.description = jsonSchema.description || jsonSchema.title
  tableOptions.timePartitioning = {
    type: 'DAY',
    requirePartitionFilter: true,
  }

  logger.info('Checking if table exists...')
  const tableExists = await gbq.tableExists(project, datasetName, tableName)
  if (tableExists) {
    logger.info('Patching table:', tableName)
    await gbq.patchTable(project, datasetName, tableName, tableOptions)
  } else {
    logger.info('Creating table:', tableName)
    await gbq.createTable(project, datasetName, tableName, tableOptions)
  }
  logger.info('Finished')
}

jsbq.run = async () => {
  const readFile = promisify(fs.readFile)

  const argv = yargs(hideBin(process.argv))
    .usage(
      'Usage: $0 -p [project] -d [dataset] -j <json schema file> --preventAdditionalObjectProperties --continueOnError',
    )
    .version(false)
    .help(false)
    .options({
      j: {
        describe: 'JSON schema file',
        demandOption: true,
        type: 'string',
        requiresArg: true,
      },
      p: {
        describe: 'GCP project name',
        type: 'string',
        requiresArg: true,
      },
      d: {
        describe: 'BigQuery dataset name',
        type: 'string',
        requiresArg: true,
      },
      preventAdditionalObjectProperties: {
        describe: 'boolean, check for additional object properties in schemas.',
        type: 'boolean',
        default: false,
      },
      continueOnError: {
        describe:
          'boolean, if error in json schema, skip element in big query schema and continue.',
        type: 'boolean',
        default: false,
      },
      debug: {
        describe: 'Enable debug logging',
        type: 'boolean',
        default: false,
      },
    })
    .parse()

  if (argv.debug) {
    logger.level = 'debug'
    logger.debug('Debug mode enabled')
  }

  const schemaData = await readFile(argv.j)
  const options = {
    preventAdditionalObjectProperties: argv.preventAdditionalObjectProperties,
    continueOnError: argv.continueOnError,
  }
  const jsonSchema = JSON.parse(schemaData)
  logger.debug('Input schema: ', jsonSchema)
  return jsbq.process(argv.p, argv.d, jsonSchema, options)
}

jsbq.run().catch((e) => {
  logger.error(e)
  process.exit(1)
})
