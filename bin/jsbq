#!/usr/bin/env node

const jsbq = module.exports = {}

const gbq = require('../src/gbq')
const { get_table_id } = require('../src/utils')
const converter = require('../src/converter')
const { logger } = require('../src/log')
const fs = require('fs')
const { promisify } = require('util')

jsbq.process = async (project, datasetName, jsonSchema, options) => {
  logger.info('Processing JSON schema...')
  const tableOptions = converter.run(jsonSchema, options)
  logger.info('Generated BigQuery schema:')
  console.log(JSON.stringify(tableOptions.schema.fields))

  if (!project && !datasetName) {
    return logger.info('Skipping table operations')
  }

  const schemaId = jsonSchema.$id || jsonSchema.id
  logger.info('Extracting table name from schema ID:', schemaId)
  const tableName = get_table_id(schemaId)
  logger.info('Table name:', tableName)

  logger.info('Setting table options...')
  tableOptions.friendly_name = jsonSchema.title
  tableOptions.description = jsonSchema.description || jsonSchema.title
  tableOptions.timePartitioning = {
    'type': 'DAY',
    'requirePartitionFilter': true
  }

  logger.info('Checking if table exists...')
  const tableExists = await gbq.tableExists(project, datasetName, tableName)
  if (tableExists) {
    logger.info('Patching table:', tableName)
    await gbq.patchTable(project, datasetName, tableName, tableOptions)
  } else {
    logger.info('Creating table:', tableName)
    await gbq.createTable(project, datasetName, tableName, tableOptions)
  }
  logger.info('Finished')
}

jsbq.run = async () => {
  const readFile = promisify(fs.readFile)

  const argv = require('yargs')
  .usage('Usage: $0 -p [project] -d [dataset] -j <json schema file> --preventAdditionalObjectProperties --continueOnError')
  .options({
    j: {
      describe: 'JSON schema file',
      demandOption: true
    },
    p: {
      describe: 'GCP project name'
    },
    d: {
      describe: 'BigQuery dataset name'
    },
    preventAdditionalObjectProperties: {
      describe: 'boolean, check for additional object properties in schemas.',
      type: 'boolean',
      default: false
    },
    continueOnError: {
      describe: 'boolean, if error in json schema, skip element in big query schema and continue.',
      type: 'boolean',
      default: false
    },
    debug: {
      describe: 'Enable debug logging',
      type: 'boolean',
      default: false
    }
  })
  .argv

  if (argv.debug) {
    logger.level = 'debug'
    logger.debug('Debug mode enabled')
  }

  const schemaData = await readFile(argv.j)
  const options = {
    preventAdditionalObjectProperties: argv.preventAdditionalObjectProperties,
    continueOnError: argv.continueOnError
  }
  const jsonSchema = JSON.parse(schemaData)
  logger.debug('Input schema: ', jsonSchema)
  return jsbq.process(argv.p, argv.d, jsonSchema, options)
}

jsbq.run().catch(e => {
  logger.error(e)
  process.exit(1)
})
